{"cells":[{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.param import Param, Params"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65e8981c-8b3d-42f5-936e-43655b362240"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.param import Param, Params\n\n# Prepare training data from a list of (label, features) tuples.\ntraining = sqlContext.createDataFrame([\n    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr = LogisticRegression(maxIter=10, regParam=0.01)\n# Print out the parameters, documentation, and any default values.\nprint \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\n\n# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1 = lr.fit(training)\n\n# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint \"Model 1 was fit using parameters: \"\nprint model1.extractParamMap()\n\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {lr.maxIter: 20}\nparamMap[lr.maxIter] = 30 # Specify 1 Param, overwriting the original maxIter.\nparamMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) # Specify multiple Params.\n\n# You can combine paramMaps, which are python dictionaries.\nparamMap2 = {lr.probabilityCol: \"myProbability\"} # Change output column name\nparamMapCombined = paramMap.copy()\nparamMapCombined.update(paramMap2)\n\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2 = lr.fit(training, paramMapCombined)\nprint \"Model 2 was fit using parameters: \"\nprint model2.extractParamMap()\n\n# Prepare test data\ntest = sqlContext.createDataFrame([\n    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n\n# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction = model2.transform(test)\nselected = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\")\nfor row in selected.collect():\n    print row"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d81cdf62-b69e-4bd4-8fe1-ee69be9edf83"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-3649359898876271>\"\u001B[0;36m, line \u001B[0;32m15\u001B[0m\n\u001B[0;31m    print \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\u001B[0m\n\u001B[0m          ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-3649359898876271>, line 15)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-3649359898876271>\"\u001B[0;36m, line \u001B[0;32m15\u001B[0m\n\u001B[0;31m    print \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\u001B[0m\n\u001B[0m          ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\n\n# Prepare training documents from a list of (id, text, label) tuples.\nLabeledDocument = Row(\"id\", \"text\", \"label\")\ntraining = sqlContext.createDataFrame([\n    (0L, \"a b c d e spark\", 1.0),\n    (1L, \"b d\", 0.0),\n    (2L, \"spark f g h\", 1.0),\n    (3L, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(training)\n\n# Prepare test documents, which are unlabeled (id, text) tuples.\ntest = sqlContext.createDataFrame([\n    (4L, \"spark i j k\"),\n    (5L, \"l m n\"),\n    (6L, \"mapreduce spark\"),\n    (7L, \"apache hadoop\")], [\"id\", \"text\"])\n\n# Make predictions on test documents and print columns of interest.\nprediction = model.transform(test)\nselected = prediction.select(\"id\", \"text\", \"prediction\")\nfor row in selected.collect():\n    print(row)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de0e3589-2ad3-46de-bb8e-9b7a69f4152d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pipeline","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1924590745769551}},"nbformat":4,"nbformat_minor":0}
